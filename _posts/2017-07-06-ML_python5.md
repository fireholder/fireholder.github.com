---
title: "Python 机器学习 Note 5"
tags: [Python]
---



# 监督学习之回归

## 线性回归

线性回归(Linear Regression)是利用数理统计中回归分析,
来确定两种或两种以上变量间相互依赖的定量关系的一种统计分
析方法。

线性回归利用称为线性回归方程的最小平方函数对一个或多个自
变量和因变量之间关系进行建模。这种函数是一个或多个称为回
归系数的模型参数的线性组合。只有一个自变量的情况称为简单
回归,大于一个自变量情况的叫做多元回归。

使用形如y=w<sup>T</sup> x+b
的线性模型拟合数据输入和输出之
间的映射关系的。

### sklearn.linear_model.LinearRegression

#### 使用方法

{% highlight python %}

from sklearn import linear_model
linear = linear_model.LinearRegression()        
#调用线性回归模块,建立回归方程
linear.fit(datasets_X, datasets_Y)            
#拟合数据

{% endhighlight %}


#### linear_model.LinearRegression()所需参数

* fit_intercept : 布尔型参数,表示是否计算该模型截距。可选参数。
* normalize : 布尔型参数,若为True,则X在回归前进行归一化。可选参数。默认值为False。
* copy_X : 布尔型参数,若为True,则X将被复制;否则将被覆盖。可选参数。默认值为True。
* n_jobs : 整型参数,表示用于计算的作业数量;若为-1,则用所有的CPU。可选参数。默认值为1。

#### linear.fit(X,y, sample_weight=None)所需参数

* X : X为训练向量;
* y : y为相对于X的目标向量;
* sample_weight : 分配给各个样本的权重数组,一般不需要使用,可省略。

#### 实例

{% highlight python %}
import matplotlib.pyplot as plt
import numpy as np
from sklearn import linear_model


datasets_X = []
datasets_Y = []
fr = open('prices.txt','r')
lines = fr.readlines()
for line in lines:
    items = line.strip().split(',')
    datasets_X.append(int(items[0]))
    datasets_Y.append(int(items[1]))

length = len(datasets_X)
datasets_X = np.array(datasets_X).reshape([length,1])
datasets_Y = np.array(datasets_Y)

minX = min(datasets_X)
maxX = max(datasets_X)
X = np.arange(minX,maxX).reshape([-1,1])


linear = linear_model.LinearRegression()
linear.fit(datasets_X, datasets_Y)

plt.scatter(datasets_X, datasets_Y, color = 'red')
plt.plot(X, linear.predict(X), color = 'blue')
plt.xlabel('Area')
plt.ylabel('Price')
plt.show()
{% endhighlight %}

## 多项式回归

多项式回归(Polynomial Regression)是研究一个因变量与一
个或多个自变量间多项式的回归分析方法。如果自变量只有一个
时,称为一元多项式回归;如果自变量有多个时,称为多元多项
式回归。


* 一元m次多项式回归方程为:
<center>y=b<sub>0</sub>+b<sub>1</sub>x+b<sub>2</sub>x<sup>2</sup>+...b<sub>m</sub>x<sup>m</sup></center>

* 二元二次多项式回归方程为:
<center>y=b<sub>0</sub>+b<sub>1</sub>x<sub>1</sub>+b<sub>2</sub>x<sub>2</sub>+b<sub>3</sub>x<sub>1</sub><sup>2</sup>+b<sub>4</sub>x<sub>2</sub><sup>2</sup>+b<sub>5</sub>x<sub>1</sub>x<sub>2</sub></center>

在一元回归分析中,如果依变量y与自变量x的关系为非线性的,但是又找不到适当的函数曲线来拟合,则可以采用一元多项式回归。

多项式回归的最大优点就是可以通过增加x的高次项对实测点进行逼
近,直至满意为止。

不同于线性回归,
多项式回归是使用曲线拟合数据的
输入与输出的映射关系。

### sklearn.preprocessing.PolynomialFeatures

#### 使用方法

{% highlight python %}
from sklearn import linear_model            
#导入线性模型
from sklearn.preprocessing import PolynomialFeatures             
#导入多项式特征构造模块

poly_reg = PolynomialFeatures(degree = 2)             
#degree=2表示建立datasets_X的二次多项式特征X_poly。
X_poly = poly_reg.fit_transform(datasets_X)              
lin_reg_2 = linear_model.LinearRegression()            
#创建线性回归,使用线性模型学习X_poly
lin_reg_2.fit(X_poly, datasets_Y)

{% endhighlight %}


## 岭回归

对于一般地线性回归问题,参数的求解采用的是最小二乘法。其目标函数如下：

<center>argmin ||Xw − y||<sup>2</sup></center>
<center>w=(X<sup>T</sup>X)<sup>−1</sup>X<sup>T</sup>y </center>

对于矩阵X,若某些列线性相关性较大(即训练样本中某些属性线性相
关),就会导致X<sup>T</sup>X的值接近0,在计算 (X<sup>T</sup>X)<sup>−1</sup>
时就会出现不稳定性。

岭回归的优化目标:

<center>argmin ||Xw − y||<sup>2</sup>+a||w||<sup>2</sup></center>

<center>w=(X<sup>T</sup>X+aI)<sup>−1</sup>X<sup>T</sup>y </center>

* 岭回归(ridge regression)是一种专用于共线性数据分析的有偏估计回归方
法
* 是一种改良的最小二乘估计法,对某些数据的拟合要强于最小二乘法。

### sklearn.linear_model.Ridge

#### 主要参数

* alpha:正则化因子,对应于损失函数中的α
* fit_intercept:表示是否计算截距,
* solver:设置计算参数的方法,可选参数‘auto’、‘svd’、‘sag’等

#### 使用方法

{% highlight python %}
from sklearn.linear_model import Ridge
#通过sklearn.linermodel加载岭回归方法
lf=Ridge(alpha=1.0,fit_intercept = True)
#创建岭回归实例
clf.fit(train_set_X,train_set_y)
#调用fit函数使用训练集训练回归器
clf.score(test_set_X,test_set_Y)
#利用测试集计算回归曲线的拟合优度
{% endhighlight %}

* 拟合优度,用于评价拟合好坏,最大为1,无最小值,当对所有输入都输出同一个值时,拟合优度为0


> 参考资料：中国大学mooc
