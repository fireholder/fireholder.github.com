---
title: "Python 机器学习 Note 1"
tags: [Python]
---


<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1. 无监督学习</a>
<ul>
<li><a href="#sec-1-1">1.1. 聚类</a>
<ul>
<li><a href="#sec-1-1-1">1.1.1. 距离</a></li>
<li><a href="#sec-1-1-2">1.1.2. Sklearn (scikit-learn)</a></li>
<li><a href="#sec-1-1-2-1">1.1.2.1 K-means 方法 </a></li>
<li><a href="#sec-1-1-2-2">1.1.2.2 DBSCAN 密度聚类</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

# 无监督学习<a id="sec-1" name="sec-1"></a>

利用无标签的数据学习数据的分布或数据与数据之间的关系被称作无监督学习。
无监督学习最常应用的场景是聚类(clustering)和降维(DimensionReduction)。

## 聚类<a id="sec-1-1" name="sec-1-1"></a>

聚类(clustering),就是根据数据的“相似性”将数据分为多类的过程。

评估两个不同样本之间的“相似性” ,通常使用的方法就是计算两个样本之间的“距离”。使用不同的方法计算样本间的距离会关系到聚类结果的好坏。

### 距离<a id="sec-1-1-1" name="sec-1-1-1"></a>

####  欧式距离

欧氏距离是最常用的一种距离度量方法,源于欧式空间中两点的距离。

#### 曼哈顿距离
曼哈顿距离也称作“城市街区距离”,类似于在城市之中驾车行驶,从一个十字路口到另外一个十字楼口的距离。

#### 马氏距离
马氏距离表示数据的协方差距离,是一种尺度无关的度量方式。也就是说马氏距离会先将样本点的各个属性标准化,再计算样本间的距离。

#### 夹角余弦
余弦相似度用向量空间中两个向量夹角的余弦值作为衡量两个样本差异的大小。余弦值越接近1,说明两个向量夹角越接近0度,表明两个向量越相似。

### Sklearn (scikit-learn)<a id="sec-1-1-2" name="sec-1-1-2"></a>

sklearn 库提供的聚类算法函数包含在 sklearn.cluster 这个模块中。如： <span class="underline">K-Means</span>, <span class="underline">近邻传播算法</span> , <span class="underline">DBSCAN</span> 等。

#### K-means方法<a id="sec-1-1-2-1" name="sec-1-1-2-1"></a>

k-means算法以k为参数,把n个对象分成k个簇,使簇内具有较高的相似度,而簇间的相似度较低。

处理过程如下：

1.  随机选择k个点作为初始的聚类中心;
2.  对于剩下的点,根据其与聚类中心的距离,将其归入最近的簇
3.  对每个簇,计算所有点的均值作为新的聚类中心
4.  重复2、3直到聚类中心不再发生改变

##### 使用方法 ：

    {% highlight python %}
    import numpy as np
    from sklearn.cluster import KMeans
    data,cityName = loadData('city.txt')          //加载数据
    km = KMeans(n<sub>clusters</sub>=3)                     //创建K-means算法实例
    label = km.fit<sub>predict</sub>(data)                  //进行训练，获得标签
    expenses = np.sum(km.cluster<sub>centers</sub>\_,axis=1)

    {% endhighlight %}

##### 参数

* n<sub>clusters</sub>:用于指定聚类中心的个数。
* label：聚类后各数据所属的标签
* axis: 按行求和
* fit<sub>predict</sub>():计算簇中心以及为簇分配序号

##### 补充
计算两条数据相似性时,Sklearn 的K-Means默认用的是欧式距离。

##### 实例

        {% highlight python %}
        \\#-\*-coding:UTF-8-\*-
        import numpy as np
        from sklearn.cluster import KMeans

        def loadData(filePath):
            fr = open(filePath,'r+',encoding="gb2312")
            lines = fr.readlines()
            retData = []
            retCityName = []
            for line in lines:
                items = line.strip().split(",")
                retCityName.append(items)
                retData.append([float(items[i]) for i in range(1,len(items))])
            return retData,retCityName

        if <span class="underline"><span class="underline">name</span></span> == '<span class="underline"><span class="underline">main</span></span>':
            data,cityName = loadData('city.txt')
            km = KMeans(n<sub>clusters</sub>=4)
            label = km.fit<sub>predict</sub>(data)
            expenses = np.sum(km.cluster<sub>centers</sub>\_,axis=1)
            #print(expenses)
            CityCluster = [[],[],[],[]]
            for i in range(len(cityName)):
                CityCluster[label[i]].append(cityName[i])
            for i in range(len(CityCluster)):
                print("Expenses:%.2f" % expenses[i])
                print(CityCluster[i])

        {% endhighlight %}

#### DBSCAN 密度聚类<a id="sec-1-1-2-2" name="sec-1-1-2-2"></a>

DBSCAN算法是一种基于密度的聚类算法:
* 聚类的时候不需要预先指定簇的个数
* 最终的簇的个数不定

DBSCAN算法将数据点分为三类:
* <span class="underline">核心点</span> :在半径Eps内含有超过MinPts数目的点
* <span class="underline">边界点</span> :在半径Eps内点的数量小于MinPts,但是落在核心点的邻域内
* <span class="underline">噪音点</span> :既不是核心点也不是边界点的点

##### 算法流程

1.  将所有点标记为核心点、边界点或噪声点;
2.  删除噪声点;
3.  为距离在Eps之内的所有核心点之间赋予一条边;
4.  每组连通的核心点形成一个簇;
5.  将每个边界点指派到一个与之关联的核心点的簇中(哪一个核心点的半径范围之内)。

##### 使用

    {% highlight python %}
    import numpy as np
    from sklearn.cluster import DBSCAN

    sklearn.cluster.DBSCAN(eps=0.5, min<sub>samples</sub>=5, metric='euclidean')
    {% endhighlight %}

##### 参数        

* eps:两个样本被看作邻居节点的最大距离
* min<sub>samples</sub>: 簇的样本数
* metric:距离计算方式

##### 实例

    {% highlight python %}
    import numpy as np
    import sklearn.cluster as skc
    from sklearn import metrics
    import matplotlib.pyplot as plt

    mac2id=dict()
    onlinetimes=[]
    f=open('TestData.txt',encoding='utf-8')
    for line in f:
        mac=line.split(',')
        onlinetime=int(line.split(','))
        starttime=int(line.split(',').split(' ').split(':'))
        if mac not in mac2id:
            mac2id[mac]=len(onlinetimes)
            onlinetimes.append((starttime,onlinetime))
        else:
            onlinetimes[mac2id[mac]]=[(starttime,onlinetime)]
    real<sub>X</sub>=np.array(onlinetimes).reshape((-1,2))

    X=real<sub>X[</sub>:,0:1]

    db=skc.DBSCAN(eps=0.01,min<sub>samples</sub>=20).fit(X)
    labels = db.labels\_

    print('Labels:')
    print(labels)
    raito=len(labels[labels[:] == -1]) / len(labels)
    print('Noise raito:',format(raito, '.2%'))

    n<sub>clusters</sub>\_ = len(set(labels)) - (1 if -1 in labels else 0)

    print('Estimated number of clusters: %d' % n<sub>clusters</sub>\_)
    print("Silhouette Coefficient: %0.3f"% metrics.silhouette<sub>score</sub>(X, labels))

    for i in range(n<sub>clusters</sub>\_):
        print('Cluster ',i,':')
        print(list(X[labels == i].flatten()))

    plt.hist(X,24)
    plt.show()

    {% endhighlight %}
