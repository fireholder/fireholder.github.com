---
title: "Python 机器学习 Note 2"
tags: [Python]

---


# 无监督学习之降维

降维,就是在保证数据所具有的代表性特性或者分布的情况下,将高维数据转化为低维数据的过程。

降维过程也可以被理解为对数据集的组成成份进行分解(decomposition)的过程,因此sklearn为降维模块命名为decomposition, 在对降维算法调用需要使用sklearn.decomposition模块。

| 算法 | 参数   | 可扩展性| 适用任务 |
| :------------- | :------------- |
|PCA |所降维度及其他超参| 大规模数据 |信号处理等
|FastICA |所降维度及其他超参 | 超大规模数据 | 图形图像特征提取|
|NMF |所降维度及其他超参 |大规模数据 |图形图像特征提取|
|LDA |所降维度及其他超参 |大规模数据 |文本数据,主题挖掘|

## 主成分分析（PCA）

主成分分析(Principal Component Analysis,PCA)是最常用的
一种降维方法,通常用于高维数据集的探索与可视化,还可以用作数
据压缩和预处理等。

PCA可以把具有相关性的高维变量合成为线性无关的低维变量,称为
主成分。主成分能够尽可能保留原始数据的信息。

### 相关术语

* 方差:是各个样本和样本均值的差的平方和的均值,用来度量一组
数据的分散程度。
* 协方差:用于度量两个变量之间的线性相关性程度,若两个变量的
协方差为0,则可认为二者线性无关。协方差矩阵则是由变量的协方差值构成的矩阵(对称阵)。
* 特征向量:矩阵的特征向量是描述数据集结构的非零向量。

### 原理

矩阵的主成分就是其协方差矩阵对应的特征向量,按照对应
的特征值大小进行排序,最大的特征值就是第一主成分,其次是第二主成分,以此类推。

### sklearn.decomposition.PCA

#### 主要参数

* n_components:指定主成分的个数,即降维后数据的维度
* svd_solver :设置特征值分解的方法,默认为‘auto’,其他可选有‘full’, ‘arpack’, ‘randomized’。

#### 实例

{% highlight python %}
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris

data = load_iris()
y = data.target
X = data.data
pca = PCA(n_components=2)
reduced_X = pca.fit_transform(X)

red_x, red_y = [], []
blue_x, blue_y = [], []
green_x, green_y = [], []

for i in range(len(reduced_X)):
    if y[i] == 0:
        red_x.append(reduced_X[i][0])
        red_y.append(reduced_X[i][1])
    elif y[i] == 1:
        blue_x.append(reduced_X[i][0])
        blue_y.append(reduced_X[i][1])
    else:
        green_x.append(reduced_X[i][0])
        green_y.append(reduced_X[i][1])

plt.scatter(red_x, red_y, c='r', marker='x')
plt.scatter(blue_x, blue_y, c='b', marker='D')
plt.scatter(green_x, green_y, c='g', marker='.')
plt.show()
{% endhighlight %}

## NMF(非负矩阵分解)
非负矩阵分解(Non-negative Matrix Factorization ,NMF)
是在矩阵中所有元素均为非负数约束条件之下的矩阵分解方法。

_基本思想_ :给定一个非负矩阵V,NMF能够找到一个非负矩阵W和一个
非负矩阵H,使得矩阵W和H的乘积近似等于矩阵V中的值。

* W矩阵:基础图像矩阵,相当于从原
矩阵V中抽取出来的特征
* H矩阵:系数矩阵。

矩阵分解优化目标:最小化W矩阵H矩阵的乘积和原始矩阵之间的差
别。

### sklearn.decomposition.NMF

#### 主要参数

* n_components:用于指定分解后矩阵的单个维度k;
* init:W矩阵和H矩阵的初始化方式,默认为‘nndsvdar’。

#### 实例

{% highlight python %}
from numpy.random import RandomState
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_olivetti_faces
from sklearn import decomposition


n_row, n_col = 2, 3
n_components = n_row * n_col
image_shape = (64, 64)


###############################################################################
# Load faces data
dataset = fetch_olivetti_faces(shuffle=True, random_state=RandomState(0))
faces = dataset.data

###############################################################################
def plot_gallery(title, images, n_col=n_col, n_row=n_row):
    plt.figure(figsize=(2. * n_col, 2.26 * n_row))
    plt.suptitle(title, size=16)

    for i, comp in enumerate(images):
        plt.subplot(n_row, n_col, i + 1)
        vmax = max(comp.max(), -comp.min())

        plt.imshow(comp.reshape(image_shape), cmap=plt.cm.gray,
                   interpolation='nearest', vmin=-vmax, vmax=vmax)
        plt.xticks(())
        plt.yticks(())
    plt.subplots_adjust(0.01, 0.05, 0.99, 0.94, 0.04, 0.)


plot_gallery("First centered Olivetti faces", faces[:n_components])
###############################################################################

estimators = [
    ('Eigenfaces - PCA using randomized SVD',
         decomposition.PCA(n_components=6,whiten=True)),

    ('Non-negative components - NMF',
         decomposition.NMF(n_components=6, init='nndsvda', tol=5e-3))
]

###############################################################################

for name, estimator in estimators:
    print("Extracting the top %d %s..." % (n_components, name))
    print(faces.shape)
    estimator.fit(faces)
    components_ = estimator.components_
    plot_gallery(name, components_[:n_components])

plt.show()

{% endhighlight %}

> 参考资料：中国大学mooc
