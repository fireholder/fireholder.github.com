---
title: "Python 机器学习 Note 3"
tags: [Python]
---

<!-- TOC depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 -->

- [监督学习的三种分类器模型](#监督学习的三种分类器模型)
	- [K近邻分类器（KNN）](#k近邻分类器knn)
		- [sklearn.neighbors.KNeighborsClassifier](#sklearnneighborskneighborsclassifier)
			- [主要参数](#主要参数)
		- [使用方法](#使用方法)
			- [有关K 的取值](#有关k-的取值)
	- [决策树](#决策树)
		- [sklearn.tree.DecisionTreeClassifier](#sklearntreedecisiontreeclassifier)
			- [主要参数](#主要参数)
			- [使用方法](#使用方法)
	- [朴素贝叶斯](#朴素贝叶斯)
		- [sklearn中的朴素贝叶斯](#sklearn中的朴素贝叶斯)
			- [主要参数](#主要参数)
			- [使用方法](#使用方法)

<!-- /TOC -->

# 监督学习的三种分类器模型

## K近邻分类器（KNN）

通过计算待分类数据点,与已有数据集中的所有数据点的距离。取
距离最小的前K个点,根据“少数服从多数“的原则,将这个数据点划分为出现次数最多的那个类别。

### sklearn.neighbors.KNeighborsClassifier

#### 主要参数

* n_neighbors:用于指定分类器中K的大小(默认值为5,注意与
kmeans的区别)
* weights:设置选中的K个点对分类结果影响的权重(默认值为平均
权重“uniform”,可以选择“distance”代表越近的点权重越高,
或者传入自己编写的以距离为参数的权重计算函数)
* algorithm:设置用于计算临近点的方法,因为当数据量很大的情况下计算当前点和所有点的距离再选出最近的k各点,这个计算量是很费时的,所以(选项中有ball_tree、kd_tree和brute,分别代表不同的寻找邻居的优化算法,默认值为auto,根据训练数据自动选择)。

### 使用方法

{% highlight python %}

X = [[0], [1], [2], [3]]      //创建一组数据 X
y = [0, 0, 1, 1]              //创建它对应的标签 y
from sklearn.neighbors import KNeighborsClassifier          //导入K 近邻分类器
neigh = KNeighborsClassifier(n_neighbors=3)                 //参数 n_neighbors 设置为 3,即使用最近的3个邻居作为分类的依据,其他参数保持默认值,并将创建好的实例赋给变量 neigh
neigh.fit(X, y)                //调用 fit() 函数,将训练数据 X 和 标签 y 送入分类器进行学习
print(neigh.predict([[1.1]]))   //调用 predict() 函数,对未知分类样本 [1.1] 分类,可以直接并将需要分类的数据构造为数组形式作为参数传入,得到分类标签作为返回值

{% endhighlight %}

样例输出值是 0,表示K近邻分类器通过计算样本 [1.1] 与训练数据的距离,取 0,1,2 这 3 个邻居作为依据,根据“投票法”最终将样本分为类别 0。


#### 有关K 的取值

* 如果较大,相当于使用较大邻域中的训练实例进行预测,可以减小估计误差,但是距离较远的样本也会对预测起作用,导致预测错误。
* 相反地,如果 K 较小,相当于使用较小的邻域进行预测,如果邻居恰好是噪声点,会导致过拟合。
* 一般情况下,K 会倾向选取较小的值,并使用交叉验证法选取最优 K 值。

## 决策树

决策树是一种树形结构的分类器,通过顺序
询问分类点的属性决定分类点最终的类别。通常
根据特征的信息增益或其他指标,构建一颗决策
树。在分类时,只需要按照决策树中的结点依次
进行判断,即可得到样本所属类别。


### sklearn.tree.DecisionTreeClassifier

#### 主要参数

* criterion :用于选择属性的准则,可以传入“gini”代表基尼
系数,或者“entropy”代表信息增益。
* max_features :表示在决策树结点进行分裂时,从多少个特征
中选择最优特征。可以设定固定数目、百分比或其他标准。它
的默认值是使用所有特征个数。

#### 使用方法

{% highlight python %}

from sklearn.datasets import load_iris          //导入 sklearn 内嵌的鸢尾花数据集
from sklearn.tree import DecisionTreeClassifier            //导入决策树分类器
from sklearn.cross_validation import cross_val_score                  //导入计算交叉验
证值的函数 cross_val_score
clf = DecisionTreeClassifier()        //使用默认参数,创建一颗基于基尼系数的决策树,并将该决策树分类器赋值给变量 clf
iris = load_iris()               //将鸢尾花数据赋值给变量 iris
cross_val_score(clf, iris.data, iris.target, cv=10)
//将决策树分类器做为待评估的模型,iris.data鸢尾花数据做为特征,iris.target鸢尾花分类标签做为目标结果,通过设定cv为10,使用10折交叉验
证。得到最终的交叉验证得分。

{% endhighlight %}

## 朴素贝叶斯

朴素贝叶斯分类器是一个以贝叶斯定理为基础
的多分类的分类器。
对于给定数据,首先基于特征的条件独立性假
设,学习输入输出的联合概率分布,然后基于此模
型,对给定的输入x,利用贝叶斯定理求出后验概
率最大的输出y。


### sklearn中的朴素贝叶斯

| 分类器 | 描述     |
| :------------- | :------------- |
| naive_bayes.GussianNB |高斯朴素贝叶斯|
|naive_bayes.MultinomialNB |针对多项式模型的朴素贝叶斯分类器|
|naive_bayes.BernoulliNB | 针对多元伯努利模型的朴素贝叶斯分类器|


#### 主要参数

* priors :给定各个类别的先验概率。如果为空,则按训练数据的实际情况进行统计;如果给定先验概率,则在训练过程中不能更改。

#### 使用方法

{% highlight python %}

import numpy
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])                 //构造训练数据 X 和 y
Y = np.array([1, 1, 1, 2, 2, 2])
from sklearn.naive_bayes import GaussianNB             //导入朴素贝叶斯分类器
clf = GaussianNB()     //创建一个高斯朴素贝叶斯分类器,并将该分类器赋给变量clf
clf.fit(X, Y)              //使用 fit() 函数进行训练
print(clf.predict([[-0.8, -1]]))        //使用 predict() 函数进行预测

{% endhighlight %}


> 参考资料：中国大学mooc
