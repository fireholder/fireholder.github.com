---
title: "Python 机器学习 Note 4"
tags: [Python]
---

<!-- TOC depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 -->

- [强化学习](#强化学习)
	- [马尔可夫决策过程（MDP）](#马尔可夫决策过程mdp)
		- [MDP基本元素](#mdp基本元素)
		- [值函数](#值函数)
	- [蒙特卡罗强化学习](#蒙特卡罗强化学习)
		- [原理](#原理)
	- [Q-learning 算法](#q-learning-算法)
	- [Deep Q Network (DQN)](#deep-q-network-dqn)
		- [学习过程](#学习过程)

<!-- /TOC -->

# 强化学习

强化学习就是程序或智能体(agent)通过与环境不断地进行交互学习一
个从环境到动作的映射,学习的目标就是使累计回报最大化。

强化学习是一种试错学习,因其在各种状态(环境)下需要尽量尝试所
有可以选择的动作,通过环境给出的反馈(即奖励)来判断动作的优劣,
最终获得环境和最优动作的映射关系(即策略)。

## 马尔可夫决策过程（MDP）

智能体agent根据当前对环境的观察采取动作获得环境的反馈,并使环境发生改变的循环过
程。

### MDP基本元素

* s∈S:有限状态state集合,s表示某个特定状态;
* a∈A:有限动作action集合,a表示某个特定动作;
* T(S, a, S’)~P r (s’|s,a): 状态转移模型, 根据当前状态s和动作a预
测下一个状态s,这里的P r 表示从s采取行动a转移到s’的概率;
* R(s,a):表示agent采取某个动作后的即时奖励,它还有 R(s,a,s’),
R(s) 等表现形式;
* Policy π(s)→a: 根据当前state来产生action,可表现为a=π(s)或
* π(a \| s) = P(a \| s),后者表示某种状态下执行某个动作的概率。

### 值函数

* _状态值函数V_ 表示执行策略π能得到的累计折扣奖励。
* _状态动作值函数Q(s,a)_ 表示在状态s下执行动作a能得到的累计折扣奖励。

在得到最优值函数之后,可以通过值函数的值得到状态s时应该采取的动作a。

## 蒙特卡罗强化学习

在现实的强化学习任务中,环境的转移概率、奖励函数往往很难得知,甚至很难得知环境中有多少状态。若学习算法不再依赖于环境建模,则称为免模型学习,蒙特卡洛强化学习就是其中的一种。

蒙特卡洛强化学习使用多次采样,然后求取平均累计奖赏作为期望累计奖赏的近似。

### 原理

直接对状态动作值函数Q(s,a)进行估计,每采样一条轨迹,就根据轨迹中的所有“状态-动作”利用下面的公式对来对
值函数进行更新。

<center>Q(s, a)= [Q(s, a) * count(s, a) + R ] /
[ count(s, a) + 1 ] </center>

每次采样更新完所有的“状态-动作”对所对应的Q(s,a),就需要
更新采样策略π。但由于策略可能是确定性的,即一个状态对应一个动作,
多次采样可能获得相同的采样轨迹,因此需要借助ε贪心策略。

## Q-learning 算法

结合了动态规划与蒙特卡洛方法的思想,使得学习更加
高效。

## Deep Q Network (DQN)

Deep Q Network(DQN):是将神经网络(neural network) 和Q-
learning结合,利用神经网络近似模拟函数Q(s,a),输入是问题的状态
(e.g.,图形),输出是每个动作a对应的Q值,然后依据Q值大小选择对
应状态执行的动作,以完成控制。

### 学习过程

1. 状态s输入,获得所有动作对应的Q
值Q(s,a);
2. 选择对应Q值最大的动作 a ′ 并执行;
3. 执行后环境发生改变,并能够获得
环境的奖励r;
4. 利用奖励r更新Q(s, a ′)——强化学习
利用新的Q(s, a′)更新网络参数—监督学习。


> 参考资料：中国大学mooc
